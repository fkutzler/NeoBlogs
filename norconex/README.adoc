= Importing data from the web with Norconex & Neo4j


== Introduction

Neo4j provides many tools for importing data, they're known as LOAD CSV from Cypher queries, but also the neo4j-admin import tool. It is also possible to import data from many other systems like Elastic Search, SQL databases, MongoDB, CouchBase using APOC procedures plugin. Finally, ETL tools like Kettle provides features aimed to reduce the data transformation effort. In short hand, the data manipulation ecosystem around Neo4j is very complete. 

And to achieve this ecosystem, now comes the time to import in Neo4J data directly comming from the Web. To do that, we need to use an external tool called Web Crawler (also known as Web Spider or Web Scraper).

== What is a Web Crawler ?

It's a program, a robot, specialized in browsing the Web, deeper and deeper. Its operation is pretty simple.

Consider the following cycle of work:

* download a Web page
* extract the links from this page to other pages and add them to the _frontier_ (a pool of URLs)
* extract content and mata-data of this page and store them somewhere

image::crawler_principle.png[crawler,300,200]



Because thereâ€™re many collectors provided by Norconex (Http, FileSystem,Sql, etc.) and many connectors where inject the data (SQL, Elastic Search, Solr, etc.). And because the solution is simple to understand (not only technical) and business oriented, really.

Into a simple configuration, you can connect an input (a data collector) to an output (a committer) and doing some transformations between them.
It acts like an ETL.
Here, I want to expose the Neo4j Committer for Norconex.

Crawling the web
====================

Crawling the web is basically simple. The mechanism consists on downloading a starting page, extract its content, and extract the links.
Each link is pushed into a queue generally known under the name of "frontier". Then, the crawler take the next link from the frontier, download the page, extract its content, extract the links and so on...

When we would to crawl the web, we have to consider some rules:
- The politness 
- The pages with interesting links
- The pages with interestig content (or both) 

The politness
=============
It is very important to understand that. We don't want "attack" a web site, we want just grabbing useful data.
And we have to be "polite" with the site, to do that:
- respect of ROBOTS.TXT
- respect of no-follow rules
- provide a think time (a pause time between two consecutive downloads) to let the web site "breath"

You can omit these rules, but if you do that, you expose yourself to be blacklisted from this web site or domain.


Pages with interesting links
============================

Sometimes, we only want the links from a web page, because the text content is not really interesting.
A Home page is usually a good example. For a clear graph, we should exclude these kind of pages.

Pages with interesting content
==============================

In counterpart, some pages have the wanted content. Then we can import this page (document) to treated by Norconex to finally push it into Neo4j.

Link the data
=============

This is probably the most difficult thing to do. Because we need to imagine a strategy to link these pages:
- url and referrer url linking strategy: and you obtain a graph of crawl, useful to see how the crawler browse the web
- metadata based linking strategy :   





