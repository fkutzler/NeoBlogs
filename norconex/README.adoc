= Importing data from the web with Norconex & Neo4j


image::committer-neo4j.png[]

== Introduction

Neo4j provides many tools for importing data, they're known as LOAD CSV from Cypher queries, but also the neo4j-admin import tool. It is also possible to import data from many other systems like Elastic Search, SQL databases, MongoDB, CouchBase using APOC procedures plugin. Finally, ETL tools like Kettle provides features aimed to reduce the data transformation effort. In short hand, the data manipulation ecosystem around Neo4j is very complete. 

And to achieve this ecosystem, now comes the time to import in Neo4J data directly from the Web. To do that, we need to use an external tool called Web Crawler (also known as Web Spider or Web Scraper).

== What is a Web Crawler ?

It's a program, a robot, specialized in browsing the Web, deeper and deeper. Its operation is pretty simple.

Consider the following cycle of work:

* download a Web page
* extract the links from this page to other pages and add them to the _frontier_ (a pool of URLs)
* extract content and meta-data of this page and store them somewhere

image::crawler_principle.png[crawler,300,200]

== Caution: politness rules

The basic principle of crawling looks like simple thing to do and it's true. However, you have to pay attention to respect politness rules. It is very important to understand that. We don't want _attack_ a web site, we want just to grab useful data.

* Give a _think time_, a time delay between two downloads, to let the target host breathing. Also avoid to download a site with more two threads
* Respect site rules like ROBOTS.TXT files or No-follow directives
* Be careful about personal data, avoid to donwload them

If you don't do that, you will incur the risk to be blacklisted or to fall down the remote server, and if we need their data, it's clearly not something we want to do.

== Norconex Web Crawler

Norconex is a small IT company from Gatineau (Quebec) specialized in the enterprise search. Norconex also propose a very nice open source Web Crawler kown as _HTTP Collector_. But, Norconex crawler is, before that, a generic pluggable crawl engine and we can see its structure on the image below.  

image::crawler_norconex.png[crawler,600,200]

Many collectors are provided by Norconex (Http, FileSystem, etc.) and many connectors where inject the data, called _committers_ (SQL, Elastic Search, Solr, etc.) are also proposed. 

From a simple XML configuration, you can connect an input (a data collector) to an output (a committer), applying filters or doing some transformations between them.

The last born of these committers is the _Neo4j committer_.

Pages with interesting links
============================

Sometimes, we only want the links from a web page, because the text content is not really interesting.
A Home page is usually a good example. For a clear graph, we should exclude these kind of pages.

Pages with interesting content
==============================

In counterpart, some pages have the wanted content. Then we can import this page (document) to treated by Norconex to finally push it into Neo4j.

Link the data
=============

This is probably the most difficult thing to do. Because we need to imagine a strategy to link these pages:
- url and referrer url linking strategy: and you obtain a graph of crawl, useful to see how the crawler browse the web
- metadata based linking strategy :   





